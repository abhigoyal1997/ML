{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from typing import List\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureType(Enum):\n",
    "    NUMERICAL = 0\n",
    "    CATEGORICAL = 1\n",
    "\n",
    "def entropy(y: np.ndarray):\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    p = counts / counts.sum()\n",
    "    return -(p * np.log2(p + 1e-9)).sum()  # Add small value to prevent log(0)\n",
    "\n",
    "def information_gain(y: np.ndarray, y_splits: List[np.ndarray]):\n",
    "    ig = entropy(y)\n",
    "    total = len(y)\n",
    "    for ys in y_splits:\n",
    "        if len(ys) == 0:  # Avoid empty splits\n",
    "            continue\n",
    "        ig -= (len(ys) / total) * entropy(ys)\n",
    "    return ig\n",
    "\n",
    "def split_at_feature(feature: np.ndarray, feature_type: FeatureType, y: np.ndarray):\n",
    "    if feature_type == FeatureType.CATEGORICAL:\n",
    "        unique_values = np.unique(feature)\n",
    "        best_ig, best_value = -1, None\n",
    "        for value in unique_values:\n",
    "            left, right = y[feature == value], y[feature != value]\n",
    "            ig = information_gain(y, [left, right])\n",
    "            if ig > best_ig:\n",
    "                best_ig, best_value = ig, value\n",
    "        return best_value, best_ig\n",
    "\n",
    "    elif feature_type == FeatureType.NUMERICAL:\n",
    "        thresholds = np.linspace(feature.min(), feature.max(), 11)[1:]\n",
    "        best_ig, best_threshold = -1, None\n",
    "        for threshold in thresholds:\n",
    "            left, right = y[feature < threshold], y[feature >= threshold]\n",
    "            ig = information_gain(y, [left, right])\n",
    "            if ig > best_ig:\n",
    "                best_ig, best_threshold = ig, threshold\n",
    "        return best_threshold, best_ig\n",
    "        \n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, num_classes: int, feature_types: List[FeatureType], max_depth: int=100, gain_threshold: float=1e-3):\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_types = feature_types\n",
    "        self.max_depth = max_depth\n",
    "        self.gain_threshold = gain_threshold\n",
    "        \n",
    "        self._distribution: np.ndarray = np.zeros(num_classes)\n",
    "        self._feature_idx: int = None\n",
    "        self._information_gain: float = 0\n",
    "        self._split_value: float = None\n",
    "        self._children: List[DecisionTree] = []\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        self._distribution[classes] = counts / len(y)\n",
    "\n",
    "        if self.max_depth == 1 or len(np.unique(y)) == 1:\n",
    "            return\n",
    "        \n",
    "        best_ig = 0\n",
    "        for i, (feature, feature_type) in enumerate(zip(X.T, self.feature_types)):\n",
    "            split_value, ig = split_at_feature(feature, feature_type, y)\n",
    "            if ig > best_ig:\n",
    "                best_ig, self._feature_idx, self._split_value = ig, i, split_value\n",
    "\n",
    "        if best_ig < self.gain_threshold or self._feature_idx is None:\n",
    "            return  # Stop splitting\n",
    "\n",
    "        # Split the data\n",
    "        feature = X[:, self._feature_idx]\n",
    "        if self.feature_types[self._feature_idx] == FeatureType.CATEGORICAL:\n",
    "            left_mask, right_mask = feature == self._split_value, feature != self._split_value\n",
    "        else:\n",
    "            left_mask, right_mask = feature < self._split_value, feature >= self._split_value\n",
    "\n",
    "        if left_mask.sum() == 0 or right_mask.sum() == 0:  # Prevent empty splits\n",
    "            return\n",
    "        \n",
    "        # Create children\n",
    "        self._children = [\n",
    "            DecisionTree(self.num_classes, self.feature_types, self.max_depth - 1, self.gain_threshold),\n",
    "            DecisionTree(self.num_classes, self.feature_types, self.max_depth - 1, self.gain_threshold)\n",
    "        ]\n",
    "        self._children[0].fit(X[left_mask], y[left_mask])\n",
    "        self._children[1].fit(X[right_mask], y[right_mask])\n",
    "    \n",
    "    def predict(self, x: np.ndarray) -> int:\n",
    "        if not self._children:\n",
    "            return self._distribution.argmax()\n",
    "        \n",
    "        if self.feature_types[self._feature_idx] == FeatureType.CATEGORICAL:\n",
    "            branch = 0 if x[self._feature_idx] == self._split_value else 1\n",
    "        else:\n",
    "            branch = 0 if x[self._feature_idx] < self._split_value else 1\n",
    "\n",
    "        return self._children[branch].predict(x)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if not self._children:\n",
    "            majority = self._distribution.argmax()\n",
    "            return f\"Leaf(class={majority}, prob={self._distribution[majority]:.2f})\"\n",
    "        \n",
    "        s = f\"Node(feature_idx={self._feature_idx}, split_value={self._split_value})\"\n",
    "        for i, child in enumerate(self._children):\n",
    "            s += \"\\n\\t\" + f\"{'Left' if i == 0 else 'Right'}: \" + repr(child).replace(\"\\n\", \"\\n\\t\")\n",
    "        \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris(as_frame=True)\n",
    "X = data[\"data\"].values\n",
    "y = data[\"target\"].values\n",
    "\n",
    "idx = list(range(len(X)))\n",
    "shuffle(idx)\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "val_size = int(len(X)*0.1)\n",
    "train_X, val_X = X[val_size:], X[:val_size]\n",
    "train_y, val_y = y[val_size:], y[:val_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTree(len(np.unique(y)), [FeatureType.NUMERICAL]*train_X.shape[1])\n",
    "classifier.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = []\n",
    "for x in val_X:\n",
    "    preds.append(classifier.predict(x))\n",
    "    \n",
    "preds = np.array(preds)\n",
    "\n",
    "(preds == val_y).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = []\n",
    "for x in train_X:\n",
    "    preds.append(classifier.predict(x))\n",
    "    \n",
    "preds = np.array(preds)\n",
    "\n",
    "(preds == train_y).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node(feature_idx=2, split_value=2.18)\n",
       "\tLeft: Leaf(class=0, prob=1.00)\n",
       "\tRight: Node(feature_idx=3, split_value=1.75)\n",
       "\t\tLeft: Node(feature_idx=2, split_value=4.96)\n",
       "\t\t\tLeft: Node(feature_idx=3, split_value=1.63)\n",
       "\t\t\t\tLeft: Leaf(class=1, prob=1.00)\n",
       "\t\t\t\tRight: Leaf(class=2, prob=1.00)\n",
       "\t\t\tRight: Node(feature_idx=3, split_value=1.52)\n",
       "\t\t\t\tLeft: Leaf(class=2, prob=1.00)\n",
       "\t\t\t\tRight: Node(feature_idx=0, split_value=6.720000000000001)\n",
       "\t\t\t\t\tLeft: Leaf(class=1, prob=1.00)\n",
       "\t\t\t\t\tRight: Leaf(class=2, prob=1.00)\n",
       "\t\tRight: Node(feature_idx=0, split_value=6.06)\n",
       "\t\t\tLeft: Node(feature_idx=1, split_value=3.0500000000000003)\n",
       "\t\t\t\tLeft: Leaf(class=2, prob=1.00)\n",
       "\t\t\t\tRight: Leaf(class=1, prob=1.00)\n",
       "\t\t\tRight: Leaf(class=2, prob=1.00)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

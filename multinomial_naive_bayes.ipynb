{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNB:\n",
    "    def __init__(self, num_classes: int, num_features: int, alpha: int=1.0):\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = num_features\n",
    "        self.alpha = alpha\n",
    "        self.log_prior = None\n",
    "        self.log_feature_probs = None\n",
    "        self.classes = None\n",
    "        \n",
    "    def fit(self, X: List[List[int]], y: List[int]):\n",
    "        self.classes, class_counts = np.unique(y, return_counts=True)\n",
    "        self.log_prior = np.log(class_counts / len(y))\n",
    "        \n",
    "        feature_counts = np.zeros([self.num_classes, self.num_features]) + self.alpha\n",
    "        for x, c in tqdm(zip(X, y), total=len(X)):\n",
    "            features, counts = np.unique(x, return_counts=True)\n",
    "            if len(features):\n",
    "                feature_counts[c, features] += counts\n",
    "\n",
    "        self.log_feature_probs = np.log(feature_counts / feature_counts.sum(axis=1, keepdims=True))\n",
    "    \n",
    "    def predict(self, x: List[int]) -> int:\n",
    "        return self.classes[(self.log_prior + self.log_feature_probs[:, x].sum(axis=1)).argmax()].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"spam_or_not_spam.csv\").sample(frac=1).dropna()\n",
    "val_size = int(len(df)*0.1)\n",
    "val_df = df.iloc[:val_size]\n",
    "train_df = df.iloc[val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.email.str.split()\n",
    "y = train_df.label.tolist()\n",
    "# X = train_df.email.str.split().apply(lambda words: list(filter(lambda w: w not in stop_words, words)))\n",
    "\n",
    "word2idx = {w: i for i,w in enumerate(set.union(*X.apply(set)), start=1)}\n",
    "word2idx[\"UNK\"] = 0\n",
    "X = X.apply(lambda words: [word2idx[w] for w in words]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958aff966d9c4a4e83887545ed6d9a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = MultinomialNB(len(np.unique(y)), len(word2idx))\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "vX = val_df.email.str.split()\n",
    "vy = val_df.label.tolist()\n",
    "# X = train_df.email.str.split().apply(lambda words: list(filter(lambda w: w not in stop_words, words)))\n",
    "\n",
    "vX = vX.apply(lambda words: [word2idx.get(w, 0) for w in words]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for x in vX:\n",
    "    preds.append(classifier.predict(x))\n",
    "    \n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9899665551839465"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds == vy).mean().item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
